{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Network Analysis Project 12\n",
    "\n",
    "Mapping Covid-19 Vaccine Discussions in a Blog Forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, you need to dump the forum thread and scrape the post data using the following:\n",
    "\n",
    "```\n",
    "$ python3 html_dump.py\n",
    "$ python3 scrape.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dump.pickle', 'rb') as f:\n",
    "    post_data = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the relationship between location and number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of posts from a location\n",
    "location_count = {}\n",
    "\n",
    "for post in post_data:\n",
    "    location = post['location']\n",
    "    location_count[location] = 1 if location not in location_count else location_count[location] + 1\n",
    "\n",
    "# Sort the amounts in descending order\n",
    "sorted_locations = sorted(location_count.keys(), key=lambda loc: location_count[loc], reverse=True)\n",
    "\n",
    "# Plot \n",
    "x = np.array(list(range(1, len(sorted_locations) + 1)))\n",
    "y = np.array(list(map(lambda loc: location_count[loc], sorted_locations)))\n",
    "plt.plot(x, y)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('logarithmic distribution of post locations and counts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account the small size of the dataset, the log-log plot looks quite linear. Based on this, the data obeys the power law distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze post lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lengths = []\n",
    "\n",
    "for post in post_data:\n",
    "    post_text = post['text']\n",
    "    post_len = len(post_text)\n",
    "    post_words = len(post_text.split())\n",
    "    print(f'length: {post_len}, words: {post_words}')\n",
    "    post_lengths.append(post_words)\n",
    "\n",
    "print(f'min: {min(post_lengths)}, max: {max(post_lengths)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a histogram of post lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_length_histogram(post_lengths):\n",
    "    plt.hist(post_lengths, bins=30, range=(0, 1300))\n",
    "    plt.title('histogram of word counts')\n",
    "\n",
    "    print(f'mean length: {np.mean(post_lengths)}')\n",
    "\n",
    "\n",
    "display_length_histogram(post_lengths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram is positively skewed, with the bulk of posts being under 200 words in length, and only some outliers above 400 words, with some in between.\n",
    "\n",
    "#### Within the top 5 regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_regions = sorted_locations[:5]\n",
    "top_region_post_lengths = []\n",
    "\n",
    "for post in post_data:\n",
    "    if post['location'] in top_regions:\n",
    "        post_text = post['text']\n",
    "        post_words = len(post_text.split())\n",
    "        top_region_post_lengths.append(post_words)\n",
    "\n",
    "display_length_histogram(top_region_post_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].hist(post_lengths, bins=30, range=(0, 1300))\n",
    "ax[0].set_title('histogram of word counts')\n",
    "ax[1].hist(top_region_post_lengths, bins=30, range=(0, 1300))\n",
    "ax[1].set_title('histogram of word counts in top regions')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both distributions look very similar. Therefore, it seems the location can not be discriminated based on the post lengths.\n",
    "\n",
    "### Quote graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for post in post_data:\n",
    "    for q in post['quotes']:\n",
    "        # Ignore self-quotes\n",
    "        if q != post['username']:\n",
    "            G.add_edge(post['username'], q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "nx.draw(G, with_labels=True, pos=nx.kamada_kawai_layout(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes and edges\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "\n",
    "# Diameter\n",
    "# diameter = nx.diameter(G)\n",
    "# This fails because the diameter is infinite (there are multiple components)\n",
    "\n",
    "# Connected components\n",
    "num_components = nx.number_connected_components(G)\n",
    "\n",
    "# Avg clustering coefficient\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "\n",
    "# Degree centrality\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "avg_degree_centrality = sum(degree_centrality.values()) / num_nodes\n",
    "\n",
    "# Closeness centrality\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "avg_closeness_centrality = sum(closeness_centrality.values()) / num_nodes\n",
    "\n",
    "print(f'Nodes: {num_nodes} Edges: {num_edges}')\n",
    "print(f'Connected components: {num_components}')\n",
    "print(f'Diameter: infinite')\n",
    "print(f'Average clustering: {avg_clustering}')\n",
    "print(f'Avg. degree centrality: {avg_degree_centrality}')\n",
    "print(f'Avg. closeness centrality: {avg_closeness_centrality}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_centrality = nx.degree_centrality(G)\n",
    "local_clustering = nx.clustering(G)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "centrality_hist = ax[0,0].hist(list(degree_centrality.values()), bins=10)\n",
    "ax[0,0].set_xlabel('degree centrality')\n",
    "ax[0,0].set_ylabel('nodes')\n",
    "ax[0,0].set_title('histogram of degree centrality')\n",
    "clustering_hist = ax[0,1].hist(list(local_clustering.values()), bins=10)\n",
    "ax[0,1].set_xlabel('clustering coefficient')\n",
    "ax[0,1].set_ylabel('nodes')\n",
    "ax[0,1].set_title('histogram of local clustering')\n",
    "\n",
    "\n",
    "def bin_centers(bins):\n",
    "    centers = []\n",
    "    for b in range(len(bins)-1):\n",
    "        centers.append((bins[b] + bins[b+1])/2)\n",
    "    return centers\n",
    "\n",
    "\n",
    "ax[1,0].plot(bin_centers(centrality_hist[1]), centrality_hist[0])\n",
    "ax[1,0].set_xlabel('degree centrality')\n",
    "ax[1,0].set_ylabel('nodes')\n",
    "ax[1,0].set_title('degree centrality log-log')\n",
    "ax[1,0].set_xscale('log')\n",
    "ax[1,0].set_yscale('log')\n",
    "ax[1,1].plot(bin_centers(clustering_hist[1]), clustering_hist[0])\n",
    "ax[1,1].set_xlabel('clustering coefficient')\n",
    "ax[1,1].set_ylabel('nodes')\n",
    "ax[1,1].set_title('local clustering log-log')\n",
    "ax[1,1].set_xscale('log')\n",
    "ax[1,1].set_yscale('log')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, accounting for the small sample size, from the log-log plots it seems that the degree centrality distribution does obey the power law, but the local clustering is a bit unclear.\n",
    "\n",
    "### Using the Girvan-Newman algorithm to find communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nx.community.girvan_newman(G)\n",
    "\n",
    "# List the community sizes in each step\n",
    "for comm in communities:\n",
    "    print(list(map(len, comm)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a certain set of users who are enaging with each other the most, as the first community remains the largest even 10 steps into the algorithm tearing it apart. This might suggest that there was, for example, a prominent conversation that many users took part in.\n",
    "\n",
    "#### Weighing users by reputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import edge_betweenness_centrality\n",
    "\n",
    "user_reputation = {}\n",
    "\n",
    "for post in post_data:\n",
    "    user_reputation[post['username']] = post['reputation']\n",
    "\n",
    "# Add weights to edges based on sum of reputation\n",
    "for u, v in G.edges:\n",
    "    # There is at least one user who is mentioned but has no posts currently in the thread\n",
    "    u_rep = user_reputation[u] if u in user_reputation else 0\n",
    "    v_rep = user_reputation[v] if v in user_reputation else 0\n",
    "    G.edges[u, v]['weight'] = u_rep + v_rep\n",
    "\n",
    "\n",
    "def most_central_edge(G):\n",
    "    centrality = edge_betweenness_centrality(G, weight=\"weight\")\n",
    "    return max(centrality, key=centrality.get)\n",
    "\n",
    "\n",
    "weighted_communities = nx.community.girvan_newman(G, most_valuable_edge=most_central_edge)\n",
    "\n",
    "# List the community sizes in each step\n",
    "for comm in weighted_communities:\n",
    "    print(list(map(len, comm)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the algorithm breaks down the first community into large chunks faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final comments\n",
    "\n",
    "Most users in the thread are not against vaccines, even though they openly lament their side effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
